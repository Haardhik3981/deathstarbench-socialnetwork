# Horizontal Pod Autoscaler (HPA) for User Service - Latency-Based Scaling
#
# WHAT THIS DOES:
# This HPA scales based on response time/latency metrics to maintain sub-500ms
# average end-to-end response time. This is ideal for performance/cost trade-off
# analysis where latency is the primary constraint.
#
# KEY CONCEPTS:
# - Latency-based scaling: Scales up when latency exceeds threshold
# - Prometheus Adapter: Required to expose Prometheus metrics to HPA
# - Multiple metrics: Can combine latency with CPU/memory for better decisions
#
# PREREQUISITES:
# - Prometheus Adapter installed (see setup-prometheus-adapter.sh)
# - Prometheus scraping metrics from services
# - Services exposing latency metrics (e.g., http_request_duration_seconds)
#
# EXPERIMENT CONFIGURATION:
# This is the "aggressive" configuration - scales up quickly to maintain <500ms
# For experiments, create copies with different minReplicas, maxReplicas, and thresholds

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: user-service-hpa-latency
  namespace: default
  labels:
    experiment: latency-based
    target-latency: "500ms"
spec:
  # Which deployment to scale
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: user-service-deployment
  
  # Replica limits - adjust for experiments
  # Experiment 1: Conservative (2-5 pods)
  # Experiment 2: Moderate (2-10 pods) - this configuration
  # Experiment 3: Aggressive (2-20 pods)
  minReplicas: 2
  maxReplicas: 10
  
  # Metrics to base scaling decisions on
  metrics:
  # Primary metric: Average response time (latency)
  # This requires Prometheus Adapter to expose Prometheus metrics
  - type: Pods
    pods:
      metric:
        name: http_request_duration_seconds
        # This metric name comes from Prometheus Adapter configuration
        # It should match the metric exposed by your services or nginx
      target:
        type: AverageValue
        # Target: 400ms average (80% of 500ms target to leave buffer)
        # Prometheus metrics are typically in seconds, so 0.4 seconds = 400ms
        averageValue: "0.4"
        # If average latency > 400ms, scale up
        # If average latency < 400ms, scale down
  
  # Secondary metric: CPU (as a safety check)
  # Prevents scaling up if CPU is already low (might indicate other issues)
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80  # Only scale if CPU is also high
  
  # Scaling behavior - optimized for latency response
  behavior:
    # Aggressive scale-up to quickly reduce latency
    scaleUp:
      stabilizationWindowSeconds: 30  # Shorter window for faster response
      policies:
      - type: Percent
        value: 100  # Double replicas if needed
        periodSeconds: 15
      - type: Pods
        value: 2    # Or add 2 pods at a time
        periodSeconds: 15
      selectPolicy: Max  # Use the policy that scales most
  
    # Conservative scale-down to prevent thrashing
    scaleDown:
      stabilizationWindowSeconds: 300  # 5 minutes - wait before scaling down
      policies:
      - type: Percent
        value: 25   # Scale down slowly (25% at a time)
        periodSeconds: 60
      selectPolicy: Min  # Use the policy that scales least

---
# Alternative: CPU/Memory-based HPA (for comparison experiments)
# Use this to compare latency-based vs resource-based scaling
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: user-service-hpa-resource
  namespace: default
  labels:
    experiment: resource-based
    comparison: baseline
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: user-service-deployment
  
  minReplicas: 2
  maxReplicas: 10
  
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      selectPolicy: Max
    
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      selectPolicy: Min

