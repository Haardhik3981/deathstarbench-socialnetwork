# Horizontal Pod Autoscaler (HPA) for User Service - Resource-Based (Baseline)
#
# WHAT THIS DOES:
# HPA automatically increases or decreases the number of pod replicas based on
# CPU and memory utilization. This is the baseline configuration for comparison
# with latency-based autoscaling.
#
# KEY CONCEPTS:
# - minReplicas: Minimum number of pods to always keep running
# - maxReplicas: Maximum number of pods that can be created
# - targetCPUUtilizationPercentage: Target CPU usage (HPA scales to maintain this)
# - scaleDown/scaleUp: Policies that control how quickly scaling happens
#
# HOW IT WORKS:
# 1. HPA continuously monitors CPU and memory metrics
# 2. If average metric exceeds target, it increases replicas
# 3. If average metric is below target, it decreases replicas
# 4. Scaling decisions respect min/max replica limits
#
# WHY WE NEED IT:
# This is the baseline configuration for performance/cost experiments.
# Compare this with latency-based HPA to see which maintains <500ms latency
# more effectively while minimizing cost.
#
# NOTE: For latency-based autoscaling, use user-service-hpa-latency.yaml instead.
# This file is kept for baseline comparison experiments.

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: user-service-hpa
  namespace: default
spec:
  # Which deployment to scale
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: user-service-deployment
  
  # Minimum and maximum number of replicas
  minReplicas: 1   # Reduced from 2 to free up cluster capacity while still allowing autoscaling
  maxReplicas: 10  # Don't create more than 10 pods (cost control)
  
  # Metrics to base scaling decisions on
  metrics:
  # Scale based on CPU utilization
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Target 70% CPU usage
        # If average CPU > 70%, scale up
        # If average CPU < 70%, scale down
  
  # Scale based on memory utilization
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # Target 80% memory usage
  
  # Optional: Scale based on custom metrics (e.g., requests per second)
  # This requires a metrics adapter (like Prometheus Adapter)
  # - type: Pods
  #   pods:
  #     metric:
  #       name: http_requests_per_second
  #     target:
  #       type: AverageValue
  #       averageValue: "100"  # Scale when average RPS > 100
  
  # Scaling behavior - controls how quickly scaling happens
  behavior:
    # Behavior when scaling up (increasing replicas)
    scaleUp:
      # Stabilization window - wait this long before scaling up again
      stabilizationWindowSeconds: 0  # Set to 0 for immediate scaling response to traffic spikes
      policies:
      - type: Percent
        value: 100  # Increase by up to 100% of current replicas
        periodSeconds: 15  # Every 15 seconds
      - type: Pods
        value: 2    # Or add 2 pods at a time
        periodSeconds: 15
      # Use the policy that results in the most pods
      selectPolicy: Max
    
    # Behavior when scaling down (decreasing replicas)
    scaleDown:
      # Reduced from 300s for faster cost optimization (scale down sooner after load decreases)
      stabilizationWindowSeconds: 60  # 1 minute (was 5 minutes)
      policies:
      - type: Percent
        value: 50   # Decrease by up to 50% of current replicas
        periodSeconds: 60  # Every 60 seconds
      # Use the policy that results in the fewest pods
      selectPolicy: Min

